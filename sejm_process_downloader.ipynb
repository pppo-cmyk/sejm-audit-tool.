{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèõÔ∏è Sejm Process Downloader - Pobieranie druku nr 471\n",
    "\n",
    "Ten notebook pobiera dane z Sejmu dla konkretnego druku legislacyjnego wraz z za≈ÇƒÖcznikami i tworzy drzewo chronologiczne oraz powiƒÖzaniowe.\n",
    "\n",
    "## U≈ºycie:\n",
    "1. Uruchom wszystkie kom√≥rki po kolei\n",
    "2. Wyniki zostanƒÖ zapisane w folderze `druk_471_dokumentacja`\n",
    "\n",
    "## Kompatybilno≈õƒá:\n",
    "- Vast.ai\n",
    "- Google Colab\n",
    "- Jupyter Notebook (lokalnie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacja wymaganych pakiet√≥w\n",
    "!pip install requests beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importy\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    HAS_BS4 = True\n",
    "    print(\"‚úÖ BeautifulSoup za≈Çadowany\")\n",
    "except ImportError:\n",
    "    HAS_BS4 = False\n",
    "    print(\"‚ö†Ô∏è BeautifulSoup nie zainstalowany\")\n",
    "\n",
    "print(\"‚úÖ Importy za≈Çadowane pomy≈õlnie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è KONFIGURACJA - ZMIE≈É WARTO≈öCI TUTAJ\n",
    "\n",
    "API_URL = \"https://api.sejm.gov.pl/sejm\"\n",
    "SEJM_WEB_URL = \"https://www.sejm.gov.pl\"\n",
    "TERM = 10  # Kadencja X\n",
    "PROCESS_NUMBER = 471  # Numer druku do pobrania\n",
    "OUTPUT_DIR = f\"druk_{PROCESS_NUMBER}_dokumentacja\"\n",
    "DOWNLOAD_ATTACHMENTS = True  # Czy pobieraƒá pliki za≈ÇƒÖcznik√≥w?\n",
    "\n",
    "print(f\"üìã Konfiguracja:\")\n",
    "print(f\"   - Kadencja: {TERM}\")\n",
    "print(f\"   - Numer druku: {PROCESS_NUMBER}\")\n",
    "print(f\"   - Folder wyj≈õciowy: {OUTPUT_DIR}\")\n",
    "print(f\"   - Pobieranie za≈ÇƒÖcznik√≥w: {'Tak' if DOWNLOAD_ATTACHMENTS else 'Nie'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa g≈Ç√≥wna\n",
    "\n",
    "class SejmProcessDownloader:\n",
    "    \"\"\"Pobiera i analizuje proces legislacyjny z Sejmu.\"\"\"\n",
    "    \n",
    "    def __init__(self, term: int, process_number: int, output_dir: str):\n",
    "        self.term = term\n",
    "        self.process_number = process_number\n",
    "        self.output_dir = output_dir\n",
    "        self.process_data: Dict[str, Any] = {}\n",
    "        self.attachments: List[Dict[str, Any]] = []\n",
    "        self.tree_structure: List[Dict[str, Any]] = []\n",
    "        self.all_prints: List[int] = []\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "    \n",
    "    def _make_request(self, url: str, timeout: int = 60) -> Optional[requests.Response]:\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "            resp = requests.get(url, timeout=timeout, headers=headers)\n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  HTTP {resp.status_code}: {url}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå B≈ÇƒÖd po≈ÇƒÖczenia: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def fetch_print_from_api(self, print_number: int) -> Optional[Dict[str, Any]]:\n",
    "        url = f\"{API_URL}/term{self.term}/prints/{print_number}\"\n",
    "        resp = self._make_request(url)\n",
    "        if resp:\n",
    "            try:\n",
    "                return resp.json()\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "    \n",
    "    def scrape_process_page(self) -> bool:\n",
    "        if not HAS_BS4:\n",
    "            print(\"‚ùå BeautifulSoup wymagany do scrapowania strony\")\n",
    "            return False\n",
    "        \n",
    "        page_url = f\"{SEJM_WEB_URL}/Sejm{self.term}.nsf/PrzebiegProc.xsp?nr={self.process_number}\"\n",
    "        print(f\"\\nüåê Pobieram stronƒô: {page_url}\")\n",
    "        \n",
    "        resp = self._make_request(page_url)\n",
    "        if not resp:\n",
    "            return False\n",
    "        \n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        title_elem = soup.find('h1') or soup.find('title')\n",
    "        if title_elem:\n",
    "            self.process_data['title'] = title_elem.get_text(strip=True)\n",
    "        else:\n",
    "            self.process_data['title'] = f\"Druk nr {self.process_number}\"\n",
    "        \n",
    "        print(f\"‚úÖ Tytu≈Ç: {self.process_data['title'][:100]}...\")\n",
    "        \n",
    "        doc_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            link_text = link.get_text(strip=True)\n",
    "            \n",
    "            if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rtf']):\n",
    "                full_url = urljoin(page_url, href)\n",
    "                doc_links.append({\n",
    "                    'url': full_url,\n",
    "                    'text': link_text,\n",
    "                    'filename': self._extract_filename(href)\n",
    "                })\n",
    "            elif 'api.sejm.gov.pl' in href:\n",
    "                doc_links.append({\n",
    "                    'url': href,\n",
    "                    'text': link_text,\n",
    "                    'filename': self._extract_filename(href)\n",
    "                })\n",
    "            elif '/druk' in href.lower() or 'druk' in link_text.lower():\n",
    "                match = re.search(r'(\\d+)', link_text)\n",
    "                if match:\n",
    "                    druk_num = int(match.group(1))\n",
    "                    if druk_num not in self.all_prints:\n",
    "                        self.all_prints.append(druk_num)\n",
    "        \n",
    "        for table in soup.find_all('table'):\n",
    "            for row in table.find_all('tr'):\n",
    "                for cell in row.find_all(['td', 'th']):\n",
    "                    for link in cell.find_all('a', href=True):\n",
    "                        href = link['href']\n",
    "                        link_text = link.get_text(strip=True)\n",
    "                        if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rtf']):\n",
    "                            full_url = urljoin(page_url, href)\n",
    "                            if full_url not in [d['url'] for d in doc_links]:\n",
    "                                doc_links.append({\n",
    "                                    'url': full_url,\n",
    "                                    'text': link_text,\n",
    "                                    'filename': self._extract_filename(href)\n",
    "                                })\n",
    "        \n",
    "        self.process_data['scraped_documents'] = doc_links\n",
    "        print(f\"üìé Znaleziono {len(doc_links)} link√≥w do dokument√≥w na stronie\")\n",
    "        \n",
    "        if self.process_number not in self.all_prints:\n",
    "            self.all_prints.insert(0, self.process_number)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _extract_filename(self, url: str) -> str:\n",
    "        parsed = urlparse(url)\n",
    "        path = unquote(parsed.path)\n",
    "        filename = os.path.basename(path)\n",
    "        if not filename or '.' not in filename:\n",
    "            filename = f\"dokument_{datetime.now().strftime('%H%M%S')}.pdf\"\n",
    "        return filename\n",
    "    \n",
    "    def fetch_process_info(self) -> bool:\n",
    "        print(f\"\\nüì• Pobieram informacje o druku nr {self.process_number}...\")\n",
    "        \n",
    "        print(f\"üîç Pr√≥bujƒô API: {API_URL}/term{self.term}/prints/{self.process_number}\")\n",
    "        print_data = self.fetch_print_from_api(self.process_number)\n",
    "        \n",
    "        if print_data:\n",
    "            self.process_data = {\n",
    "                'title': print_data.get('title', f'Druk nr {self.process_number}'),\n",
    "                'documentDate': print_data.get('documentDate', ''),\n",
    "                'deliveryDate': print_data.get('deliveryDate', ''),\n",
    "                'documentType': print_data.get('documentType', ''),\n",
    "                'prints': [self.process_number],\n",
    "                'attachments': print_data.get('attachments', []),\n",
    "                'print_data': print_data\n",
    "            }\n",
    "            self.all_prints = [self.process_number]\n",
    "            \n",
    "            additional_prints = print_data.get('additionalPrints', [])\n",
    "            if additional_prints:\n",
    "                self.all_prints.extend(additional_prints)\n",
    "            \n",
    "            print(f\"‚úÖ Znaleziono druk: {self.process_data['title'][:80]}...\")\n",
    "            print(f\"   üìé Za≈ÇƒÖczniki z API: {len(self.process_data['attachments'])}\")\n",
    "            return True\n",
    "        \n",
    "        print(\"‚ö†Ô∏è  API nie zwr√≥ci≈Ço danych, pr√≥bujƒô scrapowania strony...\")\n",
    "        if HAS_BS4:\n",
    "            if self.scrape_process_page():\n",
    "                return True\n",
    "        \n",
    "        print(f\"‚ùå Nie znaleziono druku nr {self.process_number}\")\n",
    "        return False\n",
    "    \n",
    "    def download_attachment(self, url: str, filename: str, subfolder: str = \"\") -> Optional[str]:\n",
    "        resp = self._make_request(url)\n",
    "        \n",
    "        if resp:\n",
    "            if subfolder:\n",
    "                target_dir = os.path.join(self.output_dir, subfolder)\n",
    "            else:\n",
    "                target_dir = self.output_dir\n",
    "            \n",
    "            if not os.path.exists(target_dir):\n",
    "                os.makedirs(target_dir)\n",
    "            \n",
    "            safe_filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "            filepath = os.path.join(target_dir, safe_filename)\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(resp.content)\n",
    "            \n",
    "            return filepath\n",
    "        return None\n",
    "    \n",
    "    def download_api_attachment(self, print_number: int, filename: str) -> Optional[str]:\n",
    "        url = f\"{API_URL}/term{self.term}/prints/{print_number}/{filename}\"\n",
    "        return self.download_attachment(url, filename, f\"druk_{print_number}\")\n",
    "    \n",
    "    def build_tree(self) -> List[Dict[str, Any]]:\n",
    "        tree = []\n",
    "        \n",
    "        if not self.process_data:\n",
    "            return tree\n",
    "        \n",
    "        process_node = {\n",
    "            \"level\": 0,\n",
    "            \"type\": \"PROCES\",\n",
    "            \"id\": self.process_number,\n",
    "            \"title\": self.process_data.get('title', 'Brak tytu≈Çu'),\n",
    "            \"description\": self.process_data.get('description', ''),\n",
    "            \"document_type\": self.process_data.get('documentType', ''),\n",
    "            \"document_date\": self.process_data.get('documentDate', ''),\n",
    "            \"term\": self.term,\n",
    "            \"children\": []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìã Przetwarzam {len(self.all_prints)} druk√≥w...\")\n",
    "        \n",
    "        for idx, print_num in enumerate(self.all_prints):\n",
    "            print(f\"\\nüìÑ [{idx+1}/{len(self.all_prints)}] Druk nr {print_num}...\")\n",
    "            \n",
    "            print_data = self.fetch_print_from_api(print_num)\n",
    "            \n",
    "            if print_data:\n",
    "                print_node = {\n",
    "                    \"level\": 1,\n",
    "                    \"type\": \"DRUK\",\n",
    "                    \"number\": print_num,\n",
    "                    \"title\": print_data.get('title', ''),\n",
    "                    \"document_date\": print_data.get('documentDate', ''),\n",
    "                    \"delivery_date\": print_data.get('deliveryDate', ''),\n",
    "                    \"attachments\": []\n",
    "                }\n",
    "                \n",
    "                attachments = print_data.get('attachments', [])\n",
    "                print(f\"   üìé Za≈ÇƒÖczniki: {len(attachments)}\")\n",
    "                \n",
    "                for att_idx, att in enumerate(attachments):\n",
    "                    att_node = {\n",
    "                        \"level\": 2,\n",
    "                        \"type\": \"ZA≈ÅƒÑCZNIK\",\n",
    "                        \"filename\": att,\n",
    "                        \"download_url\": f\"{API_URL}/term{self.term}/prints/{print_num}/{att}\",\n",
    "                        \"local_path\": None\n",
    "                    }\n",
    "                    \n",
    "                    if DOWNLOAD_ATTACHMENTS:\n",
    "                        print(f\"      ‚¨áÔ∏è  [{att_idx+1}/{len(attachments)}] {att}\")\n",
    "                        local_path = self.download_api_attachment(print_num, att)\n",
    "                        if local_path:\n",
    "                            att_node[\"local_path\"] = local_path\n",
    "                            print(f\"      ‚úÖ Zapisano\")\n",
    "                        else:\n",
    "                            print(f\"      ‚ùå B≈ÇƒÖd\")\n",
    "                    \n",
    "                    print_node[\"attachments\"].append(att_node)\n",
    "                    self.attachments.append(att_node)\n",
    "                \n",
    "                process_node[\"children\"].append(print_node)\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Brak danych w API\")\n",
    "        \n",
    "        scraped_docs = self.process_data.get('scraped_documents', [])\n",
    "        if scraped_docs and DOWNLOAD_ATTACHMENTS:\n",
    "            print(f\"\\nüì• Pobieranie {len(scraped_docs)} dokument√≥w ze strony Sejmu...\")\n",
    "            \n",
    "            scraped_node = {\n",
    "                \"level\": 1,\n",
    "                \"type\": \"STRONA_WWW\",\n",
    "                \"title\": \"Dokumenty ze strony Sejmu\",\n",
    "                \"attachments\": []\n",
    "            }\n",
    "            \n",
    "            for doc_idx, doc in enumerate(scraped_docs):\n",
    "                print(f\"   ‚¨áÔ∏è  [{doc_idx+1}/{len(scraped_docs)}] {doc['filename']}\")\n",
    "                \n",
    "                att_node = {\n",
    "                    \"level\": 2,\n",
    "                    \"type\": \"ZA≈ÅƒÑCZNIK_WWW\",\n",
    "                    \"filename\": doc['filename'],\n",
    "                    \"text\": doc['text'],\n",
    "                    \"download_url\": doc['url'],\n",
    "                    \"local_path\": None\n",
    "                }\n",
    "                \n",
    "                local_path = self.download_attachment(doc['url'], doc['filename'], \"strona_www\")\n",
    "                if local_path:\n",
    "                    att_node[\"local_path\"] = local_path\n",
    "                    print(f\"      ‚úÖ Zapisano\")\n",
    "                else:\n",
    "                    print(f\"      ‚ùå B≈ÇƒÖd\")\n",
    "                \n",
    "                scraped_node[\"attachments\"].append(att_node)\n",
    "                self.attachments.append(att_node)\n",
    "            \n",
    "            if scraped_node[\"attachments\"]:\n",
    "                process_node[\"children\"].append(scraped_node)\n",
    "        \n",
    "        tree.append(process_node)\n",
    "        self.tree_structure = tree\n",
    "        return tree\n",
    "    \n",
    "    def print_tree_ascii(self) -> str:\n",
    "        output_lines = []\n",
    "        \n",
    "        def add_node(node, prefix=\"\", is_last=True):\n",
    "            connector = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n",
    "            node_type = node.get(\"type\", \"\")\n",
    "            \n",
    "            if node_type == \"PROCES\":\n",
    "                title = node.get('title', 'Brak tytu≈Çu')\n",
    "                output_lines.append(f\"üìÇ DRUK NR {self.process_number}: {title[:80]}...\")\n",
    "                doc_date = node.get('document_date', '')\n",
    "                if doc_date:\n",
    "                    output_lines.append(f\"   Data dokumentu: {doc_date}\")\n",
    "                output_lines.append(f\"   Typ dokumentu: {node.get('document_type', 'N/A')}\")\n",
    "                output_lines.append(\"\")\n",
    "                \n",
    "                children = node.get(\"children\", [])\n",
    "                for idx, child in enumerate(children):\n",
    "                    is_last_child = (idx == len(children) - 1)\n",
    "                    add_node(child, \"\", is_last_child)\n",
    "                    \n",
    "            elif node_type == \"DRUK\":\n",
    "                output_lines.append(f\"{prefix}{connector}üìÑ DRUK NR {node.get('number', '?')}\")\n",
    "                title = node.get('title', '')\n",
    "                if title:\n",
    "                    output_lines.append(f\"{prefix}{'    ' if is_last else '‚îÇ   '}   Tytu≈Ç: {title[:60]}...\")\n",
    "                output_lines.append(f\"{prefix}{'    ' if is_last else '‚îÇ   '}   Data dokumentu: {node.get('document_date', 'N/A')}\")\n",
    "                output_lines.append(f\"{prefix}{'    ' if is_last else '‚îÇ   '}   Data dostarczenia: {node.get('delivery_date', 'N/A')}\")\n",
    "                \n",
    "                attachments = node.get(\"attachments\", [])\n",
    "                for att_idx, att in enumerate(attachments):\n",
    "                    is_last_att = (att_idx == len(attachments) - 1)\n",
    "                    att_prefix = prefix + (\"    \" if is_last else \"‚îÇ   \")\n",
    "                    att_connector = \"‚îî‚îÄ‚îÄ \" if is_last_att else \"‚îú‚îÄ‚îÄ \"\n",
    "                    \n",
    "                    status = \"‚úÖ\" if att.get(\"local_path\") else \"üîó\"\n",
    "                    output_lines.append(f\"{att_prefix}{att_connector}{status} {att.get('filename', '?')}\")\n",
    "                \n",
    "                output_lines.append(\"\")\n",
    "            \n",
    "            elif node_type == \"STRONA_WWW\":\n",
    "                output_lines.append(f\"{prefix}{connector}üåê DOKUMENTY ZE STRONY WWW\")\n",
    "                \n",
    "                attachments = node.get(\"attachments\", [])\n",
    "                for att_idx, att in enumerate(attachments):\n",
    "                    is_last_att = (att_idx == len(attachments) - 1)\n",
    "                    att_prefix = prefix + (\"    \" if is_last else \"‚îÇ   \")\n",
    "                    att_connector = \"‚îî‚îÄ‚îÄ \" if is_last_att else \"‚îú‚îÄ‚îÄ \"\n",
    "                    \n",
    "                    status = \"‚úÖ\" if att.get(\"local_path\") else \"üîó\"\n",
    "                    output_lines.append(f\"{att_prefix}{att_connector}{status} {att.get('filename', '?')}\")\n",
    "                \n",
    "                output_lines.append(\"\")\n",
    "        \n",
    "        for node in self.tree_structure:\n",
    "            add_node(node)\n",
    "        \n",
    "        return \"\\n\".join(output_lines)\n",
    "    \n",
    "    def generate_chronological_tree(self) -> str:\n",
    "        output_lines = []\n",
    "        output_lines.append(\"=\" * 80)\n",
    "        output_lines.append(\"üìÖ DRZEWO CHRONOLOGICZNE\")\n",
    "        output_lines.append(\"=\" * 80)\n",
    "        output_lines.append(\"\")\n",
    "        \n",
    "        events = []\n",
    "        \n",
    "        for node in self.tree_structure:\n",
    "            if node[\"type\"] == \"PROCES\":\n",
    "                for child in node.get(\"children\", []):\n",
    "                    if child.get(\"type\") == \"DRUK\":\n",
    "                        doc_date = child.get(\"document_date\", \"\")\n",
    "                        delivery_date = child.get(\"delivery_date\", \"\")\n",
    "                        \n",
    "                        if doc_date:\n",
    "                            events.append({\n",
    "                                \"date\": doc_date,\n",
    "                                \"type\": \"Dokument\",\n",
    "                                \"description\": f\"Druk nr {child.get('number', '?')}: {child.get('title', '')[:50]}...\",\n",
    "                                \"attachments\": len(child.get(\"attachments\", []))\n",
    "                            })\n",
    "                        \n",
    "                        if delivery_date and delivery_date != doc_date:\n",
    "                            events.append({\n",
    "                                \"date\": delivery_date,\n",
    "                                \"type\": \"Dostarczenie\",\n",
    "                                \"description\": f\"Dostarczenie druku nr {child.get('number', '?')}\",\n",
    "                                \"attachments\": 0\n",
    "                            })\n",
    "        \n",
    "        events.sort(key=lambda x: x.get(\"date\", \"\"))\n",
    "        \n",
    "        for event in events:\n",
    "            output_lines.append(f\"üìÜ {event['date']}\")\n",
    "            output_lines.append(f\"   [{event['type']}] {event['description']}\")\n",
    "            if event['attachments'] > 0:\n",
    "                output_lines.append(f\"   üìé Za≈ÇƒÖczniki: {event['attachments']}\")\n",
    "            output_lines.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(output_lines)\n",
    "    \n",
    "    def save_results(self):\n",
    "        print(\"\\nüíæ Zapisywanie wynik√≥w...\")\n",
    "        \n",
    "        json_path = os.path.join(self.output_dir, \"process_data.json\")\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"process\": self.process_data,\n",
    "                \"tree\": self.tree_structure,\n",
    "                \"attachments\": self.attachments,\n",
    "                \"generated_at\": datetime.now().isoformat()\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"   ‚úÖ Dane JSON: {json_path}\")\n",
    "        \n",
    "        tree_path = os.path.join(self.output_dir, \"drzewo_struktury.txt\")\n",
    "        with open(tree_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"üå≥ DRZEWO STRUKTURY PROCESU LEGISLACYJNEGO\\n\")\n",
    "            f.write(f\"   Numer druku: {self.process_number}\\n\")\n",
    "            f.write(f\"   Kadencja: {self.term}\\n\")\n",
    "            f.write(f\"   Data wygenerowania: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            f.write(self.print_tree_ascii())\n",
    "        print(f\"   ‚úÖ Drzewo struktury: {tree_path}\")\n",
    "        \n",
    "        chrono_path = os.path.join(self.output_dir, \"drzewo_chronologiczne.txt\")\n",
    "        with open(chrono_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(self.generate_chronological_tree())\n",
    "        print(f\"   ‚úÖ Drzewo chronologiczne: {chrono_path}\")\n",
    "        \n",
    "        summary_path = os.path.join(self.output_dir, \"raport_podsumowujacy.txt\")\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"üìä RAPORT PODSUMOWUJƒÑCY\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            f.write(f\"Numer druku: {self.process_number}\\n\")\n",
    "            f.write(f\"Kadencja: {self.term}\\n\")\n",
    "            f.write(f\"Tytu≈Ç: {self.process_data.get('title', 'N/A')}\\n\")\n",
    "            f.write(f\"Typ dokumentu: {self.process_data.get('documentType', 'N/A')}\\n\\n\")\n",
    "            f.write(f\"Liczba powiƒÖzanych druk√≥w: {len(self.all_prints)}\\n\")\n",
    "            f.write(f\"Liczba pobranych za≈ÇƒÖcznik√≥w: {len(self.attachments)}\\n\\n\")\n",
    "            \n",
    "            f.write(\"LINK DO STRONY SEJMU:\\n\")\n",
    "            f.write(f\"https://www.sejm.gov.pl/Sejm{self.term}.nsf/PrzebiegProc.xsp?nr={self.process_number}\\n\\n\")\n",
    "            \n",
    "            f.write(\"POBRANE ZA≈ÅƒÑCZNIKI:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for att in self.attachments:\n",
    "                status = \"‚úÖ Pobrano\" if att.get(\"local_path\") else \"‚ùå Nie pobrano\"\n",
    "                f.write(f\"  {status}: {att.get('filename', '?')}\\n\")\n",
    "                if att.get(\"local_path\"):\n",
    "                    f.write(f\"     Lokalna ≈õcie≈ºka: {att['local_path']}\\n\")\n",
    "        \n",
    "        print(f\"   ‚úÖ Raport: {summary_path}\")\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"=\" * 80)\n",
    "        print(\"üèõÔ∏è  SEJM PROCESS DOWNLOADER\")\n",
    "        print(f\"   Pobieranie druku nr {self.process_number} z kadencji {self.term}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not self.fetch_process_info():\n",
    "            print(\"\\n‚ùå Nie uda≈Ço siƒô pobraƒá informacji o druku.\")\n",
    "            return False\n",
    "        \n",
    "        self.build_tree()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üå≥ DRZEWO STRUKTURY:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(self.print_tree_ascii())\n",
    "        \n",
    "        print(self.generate_chronological_tree())\n",
    "        \n",
    "        self.save_results()\n",
    "        \n",
    "        downloaded_count = len([a for a in self.attachments if a.get('local_path')])\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ ZAKO≈ÉCZONO POMY≈öLNIE!\")\n",
    "        print(f\"   üìÇ Folder: {os.path.abspath(self.output_dir)}\")\n",
    "        print(f\"   üìÑ Pobrano dokument√≥w: {downloaded_count}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return True\n",
    "\n",
    "print(\"‚úÖ Klasa SejmProcessDownloader za≈Çadowana!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ URUCHOMIENIE POBIERANIA\n",
    "\n",
    "downloader = SejmProcessDownloader(\n",
    "    term=TERM,\n",
    "    process_number=PROCESS_NUMBER,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "downloader.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä WY≈öWIETL POBRANE DANE\n",
    "\n",
    "json_path = os.path.join(OUTPUT_DIR, \"process_data.json\")\n",
    "if os.path.exists(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"üìä PODSUMOWANIE POBRANYCH DANYCH:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Tytu≈Ç: {data['process'].get('title', 'N/A')}\")\n",
    "    print(f\"Liczba za≈ÇƒÖcznik√≥w: {len(data['attachments'])}\")\n",
    "    print(f\"\\nData wygenerowania: {data['generated_at']}\")\n",
    "else:\n",
    "    print(\"‚ùå Brak danych - uruchom najpierw kom√≥rkƒô pobierania powy≈ºej.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ LISTA POBRANYCH PLIK√ìW\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    print(f\"üìÅ Zawarto≈õƒá folderu {OUTPUT_DIR}:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "        level = root.replace(OUTPUT_DIR, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f'{indent}üìÇ {os.path.basename(root)}/')\n",
    "        sub_indent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            print(f'{sub_indent}üìÑ {file}')\n",
    "else:\n",
    "    print(f\"‚ùå Folder {OUTPUT_DIR} nie istnieje.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
